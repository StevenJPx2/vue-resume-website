---
title: Future of GPTs
---

LLMs will be so large that people will abandon more deterministic algorithms of ML models that do narrow tasks. 

In my job, that’s the biggest point of contention between the CTO and CEO. 

This was the product/MVP: creating a web app that reads a GH repo that you provide, and you will be able to chat with the repo about questions you have about it. The idea was build a generic tool that’ll help you paste any link and it’ll be able to create  chat for it. 

The CTO, a data scientist along with me, a product engineer, built the MVP in 4 days. 

The CEO then, created a GPT (cf. OpenAI DevDay) that did the same thing (arguably better) in 5 minutes. 

Meanwhile, with GitHub Universe ‘23, they’ve hedged their bets fully onto augmenting every part of the dev workflow with AI. That includes chatting with a repository. 

Now the entire business proposition of the product is essentially dead, because the literal makers of the platform we’re building our product on top of made OUR product (but much better).

That’s where the thought came in, the product we made was not great because we relied entirely on the GPT model to give us answers. Accuracy was okay, but essay-like answers are something that you cannot gauge the accuracy of. It’s more of a smoke testing, where you check the accuracy of the answer to the question based on your own bias. 

Having your company built on AI seems like a misstep in the world of OpenAI, Anthropic and more. What we should concern ourselves with is to create a kickass product, and have AI augment or enhance your workflow. 

The other point is to NOT replace every narrow ML model with more generic/general ML models like LLMs where the output could be hallucinated. 

I can envision a future where someone decides to now create a vision model for self driving but using GPT4 with Vision, to decide where a car should move. The model might be smart enough to tell you where to move, but it won’t be as smart as a narrow vision model like Tesla’s Autopilot which are mostly idempotent, the input you give in will (mostly) give you the same output. 

GPTs might in the future become so good that their output might end up being more accurate than their narrow equivalents. At that point the question becomes, will we be comfortable giving sole private vendors like OpenAI all of our private data?

